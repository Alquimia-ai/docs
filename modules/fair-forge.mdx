---
title: Fair Forge
icon: hammer
---

# Alquimia AI Fair Forge

Alquimia AI Fair Forge is a performance measurement component designed to evaluate AI models and assistants. It provides clear and useful metrics to understand and improve your AI applications through comprehensive analysis and evaluation.

## What is Fair Forge?

Fair Forge evaluates AI models and assistants through four main metrics:

- **Conversational**: Quality and effectiveness of conversations.
- **Humanity**: How natural and human-like the assistant is.
- **Bias**: Detection of biases in responses.
- **Context**: Ability to maintain and use context.

## Installation

1. Activate your virtual environment:

```shell
source venv/bin/activate
```

2. Build the package:

```shell
make package
```

3. Install the package:

```shell
pip install dist/alquimia_fair_forge-0.0.1.tar.gz -q
```

## Dataset Structure

The dataset must have the following JSON format:

```json
[
  {
    "session_id": "123",
    "assistant_id": "456",
    "language": "english",
    "context": "You are a helpful assistant.",
    "conversation": [
      {
        "qa_id": "123",
        "query": "What is Alquimia AI?",
        "ground_truth_assistant": "Is an startup that its aim is to construct assistants",
        "assistant": "I'm so happy to answer your question. Alquimia AI Is an startup dedicated to construct assistants."
      }
    ]
  }
]
```

## Using the Metrics

### Custom Retriever Example

```python
from fair_forge.schemas import Dataset
from fair_forge import Retriever
import json

class CustomRetriever(Retriever):
    def load_dataset(self) -> list[Dataset]:
        datasets = []
        with open("dataset.json") as infile:
            for dataset in json.load(infile):
                datasets.append(Dataset.model_validate(dataset))
        return datasets
```

### Available Metrics

#### Context

```python
from getpass import getpass
from fair_forge.metrics import Context
from pydantic import SecretStr

judge_api_key = SecretStr(getpass("Please enter your Judge API key: "))

metrics = Context.run(
    CustomRetriever,
    judge_api_key=judge_api_key,
    verbose=True
)
```

#### Humanity

```python
from fair_forge.metrics import Humanity

metrics = Humanity.run(
    CustomRetriever,
    verbose=True
)
```

#### Conversational

```python
from getpass import getpass
from fair_forge.metrics import Conversational
from pydantic import SecretStr

judge_api_key = SecretStr(getpass("Please enter your Judge API key: "))

metrics = Conversational.run(
    CustomRetriever,
    judge_api_key=judge_api_key,
    verbose=True
)
```

#### Bias

For this metric you need to deploy a Guardian model compatible with OpenAI-type API. IBM Granite Guardian is recommended.

```python
from getpass import getpass
from fair_forge.metrics import Bias
from pydantic import SecretStr
import os

guardian_api_key = SecretStr(getpass("Please enter your Guardian API key: "))
GUARDIAN_URL = os.environ.get("GUARDIAN_URL")
GUARDIAN_MODEL_NAME = os.environ.get("GUARDIAN_MODEL_NAME")
GUARDIAN_API_KEY = guardian_api_key

guardian_temperature = 0.7  # example
max_tokens = 512  # example

metrics = Bias.run(
    CustomRetriever,
    guardian_url=GUARDIAN_URL,
    guardian_api_key=GUARDIAN_API_KEY,
    guardian_model=GUARDIAN_MODEL_NAME,
    guardian_temperature=guardian_temperature,
    max_tokens=max_tokens,
    verbose=True
)
```

## Contributing New Metrics

1. Create a Python file in `fair_forge/metrics` with your metric.
2. Implement the class following the base structure from the fair-forge README.
3. Document your metric in `docs/journal.tex`.
4. Make a Pull Request.

---

For more details, refer to the [technical documentation](https://github.com/alquimia-ai/fair-forge) or the `journal.pdf` file in the repository. 